{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def evaluate_pairwise(runs: list, example) -> dict:\n",
    "\n",
    "    # ì ìˆ˜ ì €ì¥\n",
    "    scores = {}\n",
    "    for i, run in enumerate(runs):\n",
    "        scores[run.id] = i\n",
    "    \n",
    "\n",
    "    # ê° ì˜ˆì œì— ëŒ€í•œ ì‹¤í–‰ ìŒ\n",
    "    answer_a = runs[0].outputs[\"answer\"]\n",
    "    answer_b = runs[1].outputs[\"answer\"]\n",
    "    question = example.inputs[\"question\"]\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    grade_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an LLM judge. Compare the following two answers to a question and determine which one is better.\n",
    "        Better answer is the one that is more detailed and informative.\n",
    "        If the answer is not related to the question, it is not a good answer.\n",
    "\n",
    "        \n",
    "        # Question:\n",
    "        {question}\n",
    "        \n",
    "        #Answer A: \n",
    "        {answer_a}\n",
    "        \n",
    "        #Answer B: \n",
    "        {answer_b}\n",
    "        \n",
    "        Output should be either `A` or `B`. Pick the answer that is better.\n",
    "        \n",
    "        #Preference:\n",
    "        \"\"\"\n",
    "    )\n",
    "    answer_grader = grade_prompt | llm | StrOutputParser()\n",
    "\n",
    "    score = answer_grader.invoke(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"answer_a\": answer_a,\n",
    "            \"answer_b\": answer_b\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if score == \"A\": # Aê°€ ë” ë‹µë³€ì„ ì˜í–ˆë‹¤.\n",
    "        scores[runs[0].id] = 1\n",
    "        scores[runs[1].id] = 0\n",
    "    elif score == \"B\": # Bê°€ ë” ë‹µë³€ì„ ì˜í–ˆë‹¤.\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 1\n",
    "    else:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 0\n",
    "        \n",
    "    return {\"key\": \"ranked_preference\", \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def ask_question_with_llm(llm):\n",
    "\n",
    "    rag = PDFRAG(\n",
    "        \"data/snow-white.pdf\",\n",
    "        llm\n",
    "    )\n",
    "\n",
    "    retriever = rag.create_retriever()\n",
    "\n",
    "    rag_chain = rag.create_chain(retriever)\n",
    "\n",
    "    def _ask_question(inputs: dict):\n",
    "        context = retriever.invoke(inputs[\"question\"])\n",
    "        context = \"\\n\".join([doc.page_content for doc in context])\n",
    "        return {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"context\": context,\n",
    "            \"answer\": rag_chain.invoke(inputs[\"question\"])\n",
    "        }\n",
    "    return _ask_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d84d1dc0-cde9-46fd-bbba-69fe33a88e1f-0', usage_metadata={'input_tokens': 13, 'output_tokens': 21, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "gpt3 = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "gpt3.invoke(\"ì•ˆë…•í•˜ì„¸ìš”?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain_ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: langchain-ollama in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.14 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (0.3.14)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (0.1.137)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-ollama) (0.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.14->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.14->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.14->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.14->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\20109\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹  \\n\\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š', additional_kwargs={}, response_metadata={'model': 'gemma2', 'created_at': '2024-11-04T05:54:44.9747338Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3157001700, 'load_duration': 64104900, 'prompt_eval_count': 13, 'prompt_eval_duration': 364349000, 'eval_count': 19, 'eval_duration': 2727027000}, id='run-41bbf29a-6d7e-4c5f-ba56-0702692362bb-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´\n",
    "ollama = ChatOllama(model=\"gemma2\")\n",
    "\n",
    "# Ollama ëª¨ë¸ í˜¸ì¶œ\n",
    "ollama.invoke(\"ì•ˆë…•í•˜ì„¸ìš”?\")\n",
    "\n",
    "# ì‘ë‹µ ì¶œë ¥\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?\n",
      "A: content='ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ì‹¤ì‹œê°„ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ì—†ì–´ì„œ í˜„ì¬ ë‚ ì”¨ë¥¼ ì•Œë ¤ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚ ì”¨ í™•ì¸ì„ ìœ„í•´ ê¸°ìƒ ì›¹ì‚¬ì´íŠ¸ë‚˜ ì•±ì„ ì‚¬ìš©í•´ ë³´ì„¸ìš”! ğŸ˜Š' additional_kwargs={} response_metadata={'model': 'gemma2', 'created_at': '2024-11-04T05:55:26.1372414Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 9259345800, 'load_duration': 31052600, 'prompt_eval_count': 17, 'prompt_eval_duration': 1727131000, 'eval_count': 49, 'eval_duration': 7499222000} id='run-6c889c49-de98-4154-af91-6fa02bfd896a-0' usage_metadata={'input_tokens': 17, 'output_tokens': 49, 'total_tokens': 66}\n",
      "\n",
      "Q: ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œ?\n",
      "A: content='ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•œ ì˜ˆì¸¡ì€ í•­ìƒ í¥ë¯¸ë¡­ê³ , ë‹¤ì–‘í•œ ê°€ëŠ¥ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì €ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œì„œ ì¸ê°„ì˜ ì§€ì‹ê³¼ ì°½ì˜ë ¥ì„ ëª¨ë°©í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆì§€ë§Œ, ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³¼í•™ ê¸°ìˆ  ë°œì „ ì¶”ì„¸ì™€ í˜„ì¬ íŠ¸ë Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª‡ ê°€ì§€ ê°€ëŠ¥ì„±ì„ ì œì‹œí•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n**ê¸ì •ì ì¸ ì¸¡ë©´:**\\n\\n* **ìƒí™œì˜ ì§ˆ í–¥ìƒ:** ì¸ê³µì§€ëŠ¥ì€ ì˜ë£Œ ì§„ë‹¨, ê°œì¸ ë§ì¶¤í˜• êµìœ¡, ììœ¨ ì£¼í–‰ ê¸°ìˆ  ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ëŒë“¤ì˜ ì‚¶ì„ ë”ìš± í¸ë¦¬í•˜ê³  ì•ˆì „í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n* **í˜ì‹ ê³¼ ì°½ì¡°ì„± ì¦ëŒ€:** ì¸ê³µì§€ëŠ¥ì€ ìƒˆë¡œìš´ ì¬ë£Œ ê°œë°œ, ë””ìì¸ ì„¤ê³„, ì˜ˆìˆ  ì‘í’ˆ ì°½ì‘ ë“± ì°½ì˜ì ì¸ í™œë™ì„ ì§€ì›í•˜ì—¬ ì¸ë¥˜ì˜ ì§€ì‹ê³¼ ê¸°ìˆ  ë°œì „ì„ ì´‰ì§„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n* **íš¨ìœ¨ì„± í–¥ìƒ:** ì¸ê³µì§€ëŠ¥ì€ ë°˜ë³µì ì¸ ì‘ì—… ìë™í™”ë¥¼ í†µí•´ ì‚¬ëŒë“¤ì˜ ë…¸ë ¥ì„ ì¤„ì´ê³  ìƒì‚°ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n**ë¶€ì •ì ì¸ ì¸¡ë©´:**\\n\\n* **ê³ ìš© ê°ì†Œ:** ì¸ê³µì§€ëŠ¥ì´ ìë™í™”í•˜ëŠ” ì‘ì—…ìœ¼ë¡œ ì¸í•´ ì¼ìë¦¬ê°€ ì‚¬ë¼ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.\\n* **ë¶ˆí‰ë“± ì‹¬í™”:** ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ê³¼ ì´ë¡œë¶€í„° ì–»ëŠ” ì´ìµì— ëŒ€í•œ ì ‘ê·¼ì„± ë¶ˆê· í˜•ì€ ì‚¬íšŒì  ë¶ˆí‰ë“±ì„ ì•…í™”ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n* **ìœ¤ë¦¬ì  ë”œë ˆë§ˆ:** ì¸ê³µì§€ëŠ¥ì˜ ìœ¤ë¦¬ì ì¸ ì‚¬ìš©, ë°ì´í„° ë³´ì•ˆ ë° ê°œì¸ ì •ë³´ ë³´í˜¸ ë¬¸ì œ ë“± í•´ê²°í•´ì•¼ í•  ê³¼ì œê°€ ë§ìŠµë‹ˆë‹¤.\\n\\n**ì¤‘ìš”í•œ ê²ƒì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:**\\n\\n* **ì±…ì„ê° ìˆëŠ” ê°œë°œê³¼ í™œìš©:** ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ ì‚¬íšŒì  ì´ìµì— ê¸°ì—¬í•˜ê³  ì¸ê°„ì˜ ê¶Œë¦¬ì™€ ììœ ë¥¼ ì¡´ì¤‘í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ê°œë°œí•˜ê³  í™œìš©í•´ì•¼ í•©ë‹ˆë‹¤.\\n* **ë‹¤ì–‘ì„±ê³¼ í¬ìš©ì„± í™•ëŒ€:** ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì˜ ë‹¤ì–‘ì„±ì„ ì¦ì§„í•˜ì—¬ ëª¨ë“  ì‚¬ëŒì´ ì´ëŸ¬í•œ ê¸°ìˆ ì˜ í˜œíƒì„ ëˆ„ë¦´ ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.\\n* **ì§€ì†ì ì¸ ëŒ€í™”ì™€ ê³µê°œ ë…¼ì˜:** ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•œ ì „ë¬¸ê°€, ì •ì±… ì…ì•ˆì, ì¼ë°˜ êµ­ë¯¼ ê°„ ì§€ì†ì ì¸ ëŒ€í™”ì™€ ê³µê°œ ë…¼ì˜ë¥¼ í†µí•´ ì±…ì„ê° ìˆëŠ” ê°œë°œê³¼ í™œìš© ë°©í–¥ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\\n\\n\\nì¸ê³µì§€ëŠ¥ì€ ìš°ë¦¬ ì‚¶ì— í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. ê·¸ ê°€ëŠ¥ì„±ì€ ë¬´ê¶ë¬´ì§„í•˜ì§€ë§Œ, ë™ì‹œì— ë§ì€ ì±…ì„ê³¼ ê³¼ì œë¥¼ ì•ˆê³  ìˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë°œì „ì€ ì¸ë¥˜ê°€ ì§ë©´í•˜ëŠ” ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê³  ë” ë‚˜ì€ ë¯¸ë˜ë¥¼ ë§Œë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆì§€ë§Œ, ì´ë¥¼ ìœ„í•´ì„œëŠ” ì „ ì‚¬íšŒì  ë…¸ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'model': 'gemma2', 'created_at': '2024-11-04T05:57:57.4331318Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 151293093000, 'load_duration': 28091100, 'prompt_eval_count': 23, 'prompt_eval_duration': 2000500000, 'eval_count': 668, 'eval_duration': 149259783000} id='run-8a35cfa5-34a4-4f41-b01c-e9fb85639a68-0' usage_metadata={'input_tokens': 23, 'output_tokens': 668, 'total_tokens': 691}\n",
      "\n",
      "Q: Pythonì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "A: content='## Pythonì˜ ì¥ì \\n\\nPythonì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê°•ë ¥í•˜ê³  ìœ ì—°í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ê·¸ë ‡ê²Œ ì‚¬ë‘ë°›ëŠ” ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—¬ëŸ¬ ì¥ì  ë•Œë¬¸ì…ë‹ˆë‹¤.\\n\\n**1. ë°°ìš°ê¸° ì‰¬ì›€:** \\n\\n* ê°„ê²°í•˜ê³  ì§ê´€ì ì¸ ë¬¸ë²• êµ¬ì¡°ë¡œ, ì´ˆë³´ìê°€ë„ ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n* ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ì™€ ìœ ì‚¬í•œ êµ¬ë¬¸ì´ë¼ ì´í•´í•˜ê¸° ìš©ì´í•©ë‹ˆë‹¤.\\n\\n**2. í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í”„ë ˆì„ì›Œí¬:** \\n\\n* ì›¹ ê°œë°œ(Django, Flask), ë°ì´í„° ë¶„ì„(Pandas, NumPy), ë¨¸ì‹  ëŸ¬ë‹(Scikit-learn, TensorFlow) ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë§ì´ ì œê³µë©ë‹ˆë‹¤.\\n* ì´ë¥¼ í™œìš©í•˜ë©´ ë³µì¡í•œ ì‘ì—…ì„ ê°„ê²°í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìœ¼ë©° ê°œë°œ ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n**3. ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹° ì§€ì›:** \\n\\n* ì „ ì„¸ê³„ ë§ì€ ê°œë°œìê°€ Pythonì„ ì‚¬ìš©í•˜ê³ , í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\\n* ì˜¨ë¼ì¸ í¬ëŸ¼, ë¬¸ì„œ, êµìœ¡ ìë£Œ ë“±ì´ í’ë¶€í•˜ì—¬ ë¬¸ì œ í•´ê²° ë° í•™ìŠµì— ë„ì›€ì´ ë©ë‹ˆë‹¤.\\n\\n**4. ì˜¤í”ˆì†ŒìŠ¤ ì–¸ì–´:** \\n\\n* ë¬´ë£Œë¡œ ì‚¬ìš©, ë°°í¬, ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n* ë‹¤ì–‘í•œ í”Œë«í¼(Windows, Mac, Linux)ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.\\n\\n**5. í™•ì¥ì„±ê³¼ ìœ ì—°ì„±:** \\n\\n* ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ìŠ¤íƒ€ì¼ì„ ì§€ì›í•˜ë©°(OOP, í•¨ìˆ˜í˜•), í•„ìš”ì— ë”°ë¼ ì–¸ì–´ë¥¼ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n\\n## ê²°ë¡ ì ìœ¼ë¡œ, Pythonì€ ë°°ìš°ê¸° ì‰¬ì›€, í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬, í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°, ì˜¤í”ˆì†ŒìŠ¤ ì„±ê²© ë“± ë‹¤ì–‘í•œ ì¥ì ì´ ìˆì–´ ì´ˆë³´ìë¶€í„° ì „ë¬¸ ê°œë°œìê¹Œì§€ ì‚¬ë‘ë°›ëŠ” ì–¸ì–´ì…ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'model': 'gemma2', 'created_at': '2024-11-04T05:59:33.6309231Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 96189579300, 'load_duration': 48698600, 'prompt_eval_count': 20, 'prompt_eval_duration': 2469414000, 'eval_count': 430, 'eval_duration': 93669431000} id='run-2bea7ef5-b778-4040-a54d-f81071156314-0' usage_metadata={'input_tokens': 20, 'output_tokens': 430, 'total_tokens': 450}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions = [\n",
    "    \"ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œ?\",\n",
    "    \"Pythonì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    response = ollama.invoke(question)\n",
    "    print(f\"Q: {question}\\nA: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.invoke(\"ì•ˆë…•í•˜ì„¸ìš”?\", config={\"max_tokens\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "gpt3_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0))\n",
    "\n",
    "# ollama ì‚¬ìš©ì‹œ\n",
    "# ollama_chain = ask_question_with_llm(ChatOllama(model=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MODEL_COMPARE_EVALUATION-256328d6' at:\n",
      "https://smith.langchain.com/o/4f4b6069-9fa7-4219-889b-6de7c9b52ea4/datasets/d27aef40-c129-4c8c-85da-f16249bcb82f/compare?selectedSessions=2b3ec683-2ba4-4c2d-92af-6ff8af908451\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308f526d631b479b8308654f95be4e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MODEL_COMPARE_EVALUATION-8db0a31c' at:\n",
      "https://smith.langchain.com/o/4f4b6069-9fa7-4219-889b-6de7c9b52ea4/datasets/d27aef40-c129-4c8c-85da-f16249bcb82f/compare?selectedSessions=eecfec86-ee46-4697-bf49-15f09455b80d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36aa28a91313432da9ee27b9cee02d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    config={\"llm\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)}, # í‰ê°€ì\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"context\"],\n",
    "        \"input\": example.inputs[\"question\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_name = \"RAG_EVALUATION_DATASET\"\n",
    "\n",
    "experiment_result1 = evaluate(\n",
    "    gpt3_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator],\n",
    "    experiment_prefix=\"MODEL_COMPARE_EVALUATION\",\n",
    "    metadata={\n",
    "        \"variant\": \"GPT-3.5-turbo í‰ê°€ (cot_qa)\"\n",
    "    }\n",
    ")\n",
    "\n",
    "experiment_result2 = evaluate(\n",
    "    gpt4o_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator],\n",
    "    experiment_prefix=\"MODEL_COMPARE_EVALUATION\",\n",
    "    metadata={\n",
    "        \"variant\": \"GPT-4o-mini í‰ê°€ (cot_qa)\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/4f4b6069-9fa7-4219-889b-6de7c9b52ea4/datasets/d27aef40-c129-4c8c-85da-f16249bcb82f/compare?selectedSessions=2b3ec683-2ba4-4c2d-92af-6ff8af908451%2Ceecfec86-ee46-4697-bf49-15f09455b80d&comparativeExperiment=458c97a7-5502-4271-b8dc-a94cf8342fb1\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f6ffc43a6947f3aa4fbabefc68d28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<langsmith.evaluation._runner.ComparativeExperimentResults at 0x226b1ae00d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate_comparative\n",
    "\n",
    "evaluate_comparative(\n",
    "    [\"MODEL_COMPARE_EVALUATION-256328d6\",\"MODEL_COMPARE_EVALUATION-8db0a31c\"],\n",
    "    # í‰ê°€ì\n",
    "     evaluators=[evaluate_pairwise]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
